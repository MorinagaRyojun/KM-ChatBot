import streamlit as st
import logging
import time
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from dotenv import load_dotenv
import os

# Set up logging
logging.basicConfig(level=logging.INFO)

# Load environment variables
load_dotenv()

# Get the API key from environment variables
gemini_api_key = os.getenv("GEMINI_API_KEY")

if not gemini_api_key:
    st.error("GEMINI_API_KEY is not set. Please set it in your environment variables.")
    st.stop()

# Initialize chat history in session state
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Set up the Streamlit page
st.set_page_config(page_title="SMEGPT", page_icon="ü§ñ")
st.title("SMEGPT")

# Display chat history
for message in st.session_state.chat_history:
    with st.chat_message("Human" if isinstance(message, HumanMessage) else "AI"):
        st.markdown(message.content)

def is_related_question(new_question, chat_history, threshold=0.3):
    if not chat_history:
        return False
    
    # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    previous_questions = " ".join([msg.content for msg in chat_history if isinstance(msg, HumanMessage)])
    
    # ‡πÉ‡∏ä‡πâ TF-IDF ‡πÅ‡∏•‡∏∞ cosine similarity ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
    vectorizer = TfidfVectorizer().fit_transform([previous_questions, new_question])
    similarity = cosine_similarity(vectorizer[0:1], vectorizer[1:2])[0][0]
    
    return similarity > threshold

# Function to get general answer context from website using WebBaseLoader
def get_general_answer_context(query):
    url = "https://apex.oracle.com/pls/apex/ryo/km-api/km//"
    
    try:
        # Initialize the loader with the URL
        loader = WebBaseLoader(url)
        
        # Load data from the URL
        documents = loader.load()
        
        # Process the documents to extract relevant text
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        context = ""
        
        for document in documents:
            # WebBaseLoader returns Document objects with a 'page_content' attribute
            text = document.page_content
            splits = text_splitter.split_text(text)
            context += " ".join(splits)
        
        # Ensure the context is within acceptable size limits
        context = context[:2000]  # Limit to first 2000 characters or adjust as needed
        
        return context
    except Exception as e:
        logging.error(f"Error fetching general answer context: {e}")
        return "Error fetching context. Please try again later."

# New function to get event information
def get_event_information():
    url = "https://apex.oracle.com/pls/apex/ryo/events/from_today"
    
    try:
        # Initialize the loader with the URL
        loader = WebBaseLoader(url)
        
        # Load data from the URL
        documents = loader.load()
        
        # Process the documents to extract relevant text
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        event_info = ""
        
        for document in documents:
            # WebBaseLoader returns Document objects with a 'page_content' attribute
            text = document.page_content
            splits = text_splitter.split_text(text)
            event_info += " ".join(splits)
        
        # Ensure the event information is within acceptable size limits
        event_info = event_info[:2000]  # Limit to first 2000 characters or adjust as needed
        
        return event_info
    except Exception as e:
        logging.error(f"Error fetching event information: {e}")
        return "Error fetching event information. Please try again later."

    
# Improved function to check if the query is event-related
def is_event_query(query):
    query_lower = query.lower()
    
    event_terms = [
        "event", "exhibition", "conference", "seminar", "workshop", "meetup",
        "gathering", "festival", "concert", "show", "expo", "fair",
        "symposium", "convention", "forum", "ceremony", "gala",
        "‡∏á‡∏≤‡∏ô", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏ô‡∏¥‡∏ó‡∏£‡∏£‡∏®‡∏Å‡∏≤‡∏£", "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏™‡∏±‡∏°‡∏°‡∏ô‡∏≤", "‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏Ñ‡∏ä‡πá‡∏≠‡∏õ",
        "‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏ï‡∏±‡∏ß", "‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•", "‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ï", "‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á", "‡∏°‡∏´‡∏Å‡∏£‡∏£‡∏°", "‡∏≠‡∏≠‡∏Å‡∏£‡πâ‡∏≤‡∏ô",
        "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£", "‡∏û‡∏¥‡∏ò‡∏µ", "‡∏á‡∏≤‡∏ô‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á", "‡∏°‡∏µ‡∏ï‡∏ï‡∏¥‡πâ‡∏á", "‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå"
    ]
    
    # Calculate the TF-IDF scores for the query and event terms
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([query_lower] + event_terms)
    
    # Calculate cosine similarity between the query and event terms
    similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])
    
    # If the maximum similarity score is above a threshold, consider it an event query
    threshold = 0.3  # Adjust this value as needed
    return similarity_scores.max() > threshold

# Function to get combined context
def get_combined_context(query):
    general_context = get_general_answer_context(query)
    event_context = get_event_information()
    
    return f"General Context: {general_context}\n\nEvent Context: {event_context}"

# Updated function to get response from LLM
def get_response(query, chat_history, context):
    template = """
    ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö SME ‡πÅ‡∏•‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ
    ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:
    
    ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: {context}
    
    ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó: {chat_history}
    
    ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {user_question}
    
    ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Event Context
    ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö SME ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å General Context
    ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô ‡πÉ‡∏´‡πâ‡∏ú‡∏™‡∏°‡∏ú‡∏™‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    """
    
    try:
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro", google_api_key=gemini_api_key)
        
        prompt = ChatPromptTemplate.from_template(template)
        messages = prompt.format_messages(context=context, chat_history=chat_history, user_question=query)
        
        return llm.stream(messages)
    except Exception as e:
        logging.error(f"Error in get_response: {e}")
        return iter([AIMessage(content="‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡πÇ‡∏õ‡∏£‡∏î‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á")])
    
    
# User input
user_query = st.chat_input("‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì")
if user_query:
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if not is_related_question(user_query, st.session_state.chat_history):
        st.session_state.chat_history = []  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà
    
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    
    with st.chat_message("Human"):
        st.markdown(user_query)
    
    with st.chat_message("AI"):
        message_placeholder = st.empty()
        full_response = ""
        
        # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏£‡∏ß‡∏°
        context_text = get_combined_context(user_query)
        
        start_time = time.time()
        try:
            for chunk in get_response(user_query, st.session_state.chat_history, context_text):
                full_response += chunk.content
                message_placeholder.markdown(full_response + "‚ñå")
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏°‡∏î‡πÄ‡∏ß‡∏•‡∏≤
                if time.time() - start_time > 60:  # ‡∏´‡∏°‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏•‡∏±‡∏á 60 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
                    raise TimeoutError("‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ")
                
            message_placeholder.markdown(full_response)
        except Exception as e:
            logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {e}")
            message_placeholder.markdown("‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡πÇ‡∏õ‡∏£‡∏î‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á")
        
        if full_response:
            st.session_state.chat_history.append(AIMessage(content=full_response))
        else:
            st.warning("‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö ‡πÇ‡∏õ‡∏£‡∏î‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")

# Add a button to clear chat history
if st.button("‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"):
    st.session_state.chat_history = []
    st.experimental_rerun()